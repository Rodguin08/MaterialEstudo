{"ast":null,"code":"'use strict';\n\nvar _asyncToGenerator = require(\"C:/Users/Voluti/Desktop/Git/Cursos/Angular 2/gatitobook/node_modules/@babel/runtime/helpers/asyncToGenerator.js\").default;\n\nconst util = require('util');\n\nconst crypto = require('crypto');\n\nconst fs = require('@npmcli/fs');\n\nconst Minipass = require('minipass');\n\nconst path = require('path');\n\nconst ssri = require('ssri');\n\nconst uniqueFilename = require('unique-filename');\n\nconst contentPath = require('./content/path');\n\nconst fixOwner = require('./util/fix-owner');\n\nconst hashToSegments = require('./util/hash-to-segments');\n\nconst indexV = require('../package.json')['cache-version'].index;\n\nconst moveFile = require('@npmcli/move-file');\n\nconst _rimraf = require('rimraf');\n\nconst rimraf = util.promisify(_rimraf);\nrimraf.sync = _rimraf.sync;\nmodule.exports.NotFoundError = class NotFoundError extends Error {\n  constructor(cache, key) {\n    super(`No cache entry for ${key} found in ${cache}`);\n    this.code = 'ENOENT';\n    this.cache = cache;\n    this.key = key;\n  }\n\n};\nmodule.exports.compact = compact;\n\nfunction compact(_x, _x2, _x3) {\n  return _compact.apply(this, arguments);\n}\n\nfunction _compact() {\n  _compact = _asyncToGenerator(function* (cache, key, matchFn, opts = {}) {\n    const bucket = bucketPath(cache, key);\n    const entries = yield bucketEntries(bucket);\n    const newEntries = []; // we loop backwards because the bottom-most result is the newest\n    // since we add new entries with appendFile\n\n    for (let i = entries.length - 1; i >= 0; --i) {\n      const entry = entries[i]; // a null integrity could mean either a delete was appended\n      // or the user has simply stored an index that does not map\n      // to any content. we determine if the user wants to keep the\n      // null integrity based on the validateEntry function passed in options.\n      // if the integrity is null and no validateEntry is provided, we break\n      // as we consider the null integrity to be a deletion of everything\n      // that came before it.\n\n      if (entry.integrity === null && !opts.validateEntry) {\n        break;\n      } // if this entry is valid, and it is either the first entry or\n      // the newEntries array doesn't already include an entry that\n      // matches this one based on the provided matchFn, then we add\n      // it to the beginning of our list\n\n\n      if ((!opts.validateEntry || opts.validateEntry(entry) === true) && (newEntries.length === 0 || !newEntries.find(oldEntry => matchFn(oldEntry, entry)))) {\n        newEntries.unshift(entry);\n      }\n    }\n\n    const newIndex = '\\n' + newEntries.map(entry => {\n      const stringified = JSON.stringify(entry);\n      const hash = hashEntry(stringified);\n      return `${hash}\\t${stringified}`;\n    }).join('\\n');\n\n    const setup = /*#__PURE__*/function () {\n      var _ref5 = _asyncToGenerator(function* () {\n        const target = uniqueFilename(path.join(cache, 'tmp'), opts.tmpPrefix);\n        yield fixOwner.mkdirfix(cache, path.dirname(target));\n        return {\n          target,\n          moved: false\n        };\n      });\n\n      return function setup() {\n        return _ref5.apply(this, arguments);\n      };\n    }();\n\n    const teardown = /*#__PURE__*/function () {\n      var _ref6 = _asyncToGenerator(function* (tmp) {\n        if (!tmp.moved) {\n          return rimraf(tmp.target);\n        }\n      });\n\n      return function teardown(_x15) {\n        return _ref6.apply(this, arguments);\n      };\n    }();\n\n    const write = /*#__PURE__*/function () {\n      var _ref7 = _asyncToGenerator(function* (tmp) {\n        yield fs.writeFile(tmp.target, newIndex, {\n          flag: 'wx'\n        });\n        yield fixOwner.mkdirfix(cache, path.dirname(bucket)); // we use @npmcli/move-file directly here because we\n        // want to overwrite the existing file\n\n        yield moveFile(tmp.target, bucket);\n        tmp.moved = true;\n\n        try {\n          yield fixOwner.chownr(cache, bucket);\n        } catch (err) {\n          if (err.code !== 'ENOENT') {\n            throw err;\n          }\n        }\n      });\n\n      return function write(_x16) {\n        return _ref7.apply(this, arguments);\n      };\n    }(); // write the file atomically\n\n\n    const tmp = yield setup();\n\n    try {\n      yield write(tmp);\n    } finally {\n      yield teardown(tmp);\n    } // we reverse the list we generated such that the newest\n    // entries come first in order to make looping through them easier\n    // the true passed to formatEntry tells it to keep null\n    // integrity values, if they made it this far it's because\n    // validateEntry returned true, and as such we should return it\n\n\n    return newEntries.reverse().map(entry => formatEntry(cache, entry, true));\n  });\n  return _compact.apply(this, arguments);\n}\n\nmodule.exports.insert = insert;\n\nfunction insert(_x4, _x5, _x6) {\n  return _insert.apply(this, arguments);\n}\n\nfunction _insert() {\n  _insert = _asyncToGenerator(function* (cache, key, integrity, opts = {}) {\n    const {\n      metadata,\n      size\n    } = opts;\n    const bucket = bucketPath(cache, key);\n    const entry = {\n      key,\n      integrity: integrity && ssri.stringify(integrity),\n      time: Date.now(),\n      size,\n      metadata\n    };\n\n    try {\n      yield fixOwner.mkdirfix(cache, path.dirname(bucket));\n      const stringified = JSON.stringify(entry); // NOTE - Cleverness ahoy!\n      //\n      // This works because it's tremendously unlikely for an entry to corrupt\n      // another while still preserving the string length of the JSON in\n      // question. So, we just slap the length in there and verify it on read.\n      //\n      // Thanks to @isaacs for the whiteboarding session that ended up with\n      // this.\n\n      yield fs.appendFile(bucket, `\\n${hashEntry(stringified)}\\t${stringified}`);\n      yield fixOwner.chownr(cache, bucket);\n    } catch (err) {\n      if (err.code === 'ENOENT') {\n        return undefined;\n      }\n\n      throw err; // There's a class of race conditions that happen when things get deleted\n      // during fixOwner, or between the two mkdirfix/chownr calls.\n      //\n      // It's perfectly fine to just not bother in those cases and lie\n      // that the index entry was written. Because it's a cache.\n    }\n\n    return formatEntry(cache, entry);\n  });\n  return _insert.apply(this, arguments);\n}\n\nmodule.exports.insert.sync = insertSync;\n\nfunction insertSync(cache, key, integrity, opts = {}) {\n  const {\n    metadata,\n    size\n  } = opts;\n  const bucket = bucketPath(cache, key);\n  const entry = {\n    key,\n    integrity: integrity && ssri.stringify(integrity),\n    time: Date.now(),\n    size,\n    metadata\n  };\n  fixOwner.mkdirfix.sync(cache, path.dirname(bucket));\n  const stringified = JSON.stringify(entry);\n  fs.appendFileSync(bucket, `\\n${hashEntry(stringified)}\\t${stringified}`);\n\n  try {\n    fixOwner.chownr.sync(cache, bucket);\n  } catch (err) {\n    if (err.code !== 'ENOENT') {\n      throw err;\n    }\n  }\n\n  return formatEntry(cache, entry);\n}\n\nmodule.exports.find = find;\n\nfunction find(_x7, _x8) {\n  return _find.apply(this, arguments);\n}\n\nfunction _find() {\n  _find = _asyncToGenerator(function* (cache, key) {\n    const bucket = bucketPath(cache, key);\n\n    try {\n      const entries = yield bucketEntries(bucket);\n      return entries.reduce((latest, next) => {\n        if (next && next.key === key) {\n          return formatEntry(cache, next);\n        } else {\n          return latest;\n        }\n      }, null);\n    } catch (err) {\n      if (err.code === 'ENOENT') {\n        return null;\n      } else {\n        throw err;\n      }\n    }\n  });\n  return _find.apply(this, arguments);\n}\n\nmodule.exports.find.sync = findSync;\n\nfunction findSync(cache, key) {\n  const bucket = bucketPath(cache, key);\n\n  try {\n    return bucketEntriesSync(bucket).reduce((latest, next) => {\n      if (next && next.key === key) {\n        return formatEntry(cache, next);\n      } else {\n        return latest;\n      }\n    }, null);\n  } catch (err) {\n    if (err.code === 'ENOENT') {\n      return null;\n    } else {\n      throw err;\n    }\n  }\n}\n\nmodule.exports.delete = del;\n\nfunction del(cache, key, opts = {}) {\n  if (!opts.removeFully) {\n    return insert(cache, key, null, opts);\n  }\n\n  const bucket = bucketPath(cache, key);\n  return rimraf(bucket);\n}\n\nmodule.exports.delete.sync = delSync;\n\nfunction delSync(cache, key, opts = {}) {\n  if (!opts.removeFully) {\n    return insertSync(cache, key, null, opts);\n  }\n\n  const bucket = bucketPath(cache, key);\n  return rimraf.sync(bucket);\n}\n\nmodule.exports.lsStream = lsStream;\n\nfunction lsStream(cache) {\n  const indexDir = bucketDir(cache);\n  const stream = new Minipass({\n    objectMode: true\n  }); // Set all this up to run on the stream and then just return the stream\n\n  Promise.resolve().then( /*#__PURE__*/_asyncToGenerator(function* () {\n    const buckets = yield readdirOrEmpty(indexDir);\n    yield Promise.all(buckets.map( /*#__PURE__*/function () {\n      var _ref2 = _asyncToGenerator(function* (bucket) {\n        const bucketPath = path.join(indexDir, bucket);\n        const subbuckets = yield readdirOrEmpty(bucketPath);\n        yield Promise.all(subbuckets.map( /*#__PURE__*/function () {\n          var _ref3 = _asyncToGenerator(function* (subbucket) {\n            const subbucketPath = path.join(bucketPath, subbucket); // \"/cachename/<bucket 0xFF>/<bucket 0xFF>./*\"\n\n            const subbucketEntries = yield readdirOrEmpty(subbucketPath);\n            yield Promise.all(subbucketEntries.map( /*#__PURE__*/function () {\n              var _ref4 = _asyncToGenerator(function* (entry) {\n                const entryPath = path.join(subbucketPath, entry);\n\n                try {\n                  const entries = yield bucketEntries(entryPath); // using a Map here prevents duplicate keys from showing up\n                  // twice, I guess?\n\n                  const reduced = entries.reduce((acc, entry) => {\n                    acc.set(entry.key, entry);\n                    return acc;\n                  }, new Map()); // reduced is a map of key => entry\n\n                  for (const entry of reduced.values()) {\n                    const formatted = formatEntry(cache, entry);\n\n                    if (formatted) {\n                      stream.write(formatted);\n                    }\n                  }\n                } catch (err) {\n                  if (err.code === 'ENOENT') {\n                    return undefined;\n                  }\n\n                  throw err;\n                }\n              });\n\n              return function (_x11) {\n                return _ref4.apply(this, arguments);\n              };\n            }()));\n          });\n\n          return function (_x10) {\n            return _ref3.apply(this, arguments);\n          };\n        }()));\n      });\n\n      return function (_x9) {\n        return _ref2.apply(this, arguments);\n      };\n    }()));\n    stream.end();\n    return stream;\n  })).catch(err => stream.emit('error', err));\n  return stream;\n}\n\nmodule.exports.ls = ls;\n\nfunction ls(_x12) {\n  return _ls.apply(this, arguments);\n}\n\nfunction _ls() {\n  _ls = _asyncToGenerator(function* (cache) {\n    const entries = yield lsStream(cache).collect();\n    return entries.reduce((acc, xs) => {\n      acc[xs.key] = xs;\n      return acc;\n    }, {});\n  });\n  return _ls.apply(this, arguments);\n}\n\nmodule.exports.bucketEntries = bucketEntries;\n\nfunction bucketEntries(_x13, _x14) {\n  return _bucketEntries2.apply(this, arguments);\n}\n\nfunction _bucketEntries2() {\n  _bucketEntries2 = _asyncToGenerator(function* (bucket, filter) {\n    const data = yield fs.readFile(bucket, 'utf8');\n    return _bucketEntries(data, filter);\n  });\n  return _bucketEntries2.apply(this, arguments);\n}\n\nmodule.exports.bucketEntries.sync = bucketEntriesSync;\n\nfunction bucketEntriesSync(bucket, filter) {\n  const data = fs.readFileSync(bucket, 'utf8');\n  return _bucketEntries(data, filter);\n}\n\nfunction _bucketEntries(data, filter) {\n  const entries = [];\n  data.split('\\n').forEach(entry => {\n    if (!entry) {\n      return;\n    }\n\n    const pieces = entry.split('\\t');\n\n    if (!pieces[1] || hashEntry(pieces[1]) !== pieces[0]) {\n      // Hash is no good! Corruption or malice? Doesn't matter!\n      // EJECT EJECT\n      return;\n    }\n\n    let obj;\n\n    try {\n      obj = JSON.parse(pieces[1]);\n    } catch (e) {\n      // Entry is corrupted!\n      return;\n    }\n\n    if (obj) {\n      entries.push(obj);\n    }\n  });\n  return entries;\n}\n\nmodule.exports.bucketDir = bucketDir;\n\nfunction bucketDir(cache) {\n  return path.join(cache, `index-v${indexV}`);\n}\n\nmodule.exports.bucketPath = bucketPath;\n\nfunction bucketPath(cache, key) {\n  const hashed = hashKey(key);\n  return path.join.apply(path, [bucketDir(cache)].concat(hashToSegments(hashed)));\n}\n\nmodule.exports.hashKey = hashKey;\n\nfunction hashKey(key) {\n  return hash(key, 'sha256');\n}\n\nmodule.exports.hashEntry = hashEntry;\n\nfunction hashEntry(str) {\n  return hash(str, 'sha1');\n}\n\nfunction hash(str, digest) {\n  return crypto.createHash(digest).update(str).digest('hex');\n}\n\nfunction formatEntry(cache, entry, keepAll) {\n  // Treat null digests as deletions. They'll shadow any previous entries.\n  if (!entry.integrity && !keepAll) {\n    return null;\n  }\n\n  return {\n    key: entry.key,\n    integrity: entry.integrity,\n    path: entry.integrity ? contentPath(cache, entry.integrity) : undefined,\n    size: entry.size,\n    time: entry.time,\n    metadata: entry.metadata\n  };\n}\n\nfunction readdirOrEmpty(dir) {\n  return fs.readdir(dir).catch(err => {\n    if (err.code === 'ENOENT' || err.code === 'ENOTDIR') {\n      return [];\n    }\n\n    throw err;\n  });\n}","map":null,"metadata":{},"sourceType":"script"}