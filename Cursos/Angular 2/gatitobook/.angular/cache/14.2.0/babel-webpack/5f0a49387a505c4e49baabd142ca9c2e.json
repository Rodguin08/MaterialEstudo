{"ast":null,"code":"'use strict';\n\nvar _asyncToGenerator = require(\"C:/Users/Voluti/Desktop/Git/Cursos/Angular 2/gatitobook/node_modules/@babel/runtime/helpers/asyncToGenerator.js\").default;\n\nconst util = require('util');\n\nconst pMap = require('p-map');\n\nconst contentPath = require('./content/path');\n\nconst fixOwner = require('./util/fix-owner');\n\nconst fs = require('@npmcli/fs');\n\nconst fsm = require('fs-minipass');\n\nconst glob = util.promisify(require('glob'));\n\nconst index = require('./entry-index');\n\nconst path = require('path');\n\nconst rimraf = util.promisify(require('rimraf'));\n\nconst ssri = require('ssri');\n\nconst globify = pattern => pattern.split('\\\\').join('/');\n\nconst hasOwnProperty = (obj, key) => Object.prototype.hasOwnProperty.call(obj, key);\n\nconst verifyOpts = opts => ({\n  concurrency: 20,\n  log: {\n    silly() {}\n\n  },\n  ...opts\n});\n\nmodule.exports = verify;\n\nfunction verify(_x, _x2) {\n  return _verify.apply(this, arguments);\n}\n\nfunction _verify() {\n  _verify = _asyncToGenerator(function* (cache, opts) {\n    opts = verifyOpts(opts);\n    opts.log.silly('verify', 'verifying cache at', cache);\n    const steps = [markStartTime, fixPerms, garbageCollect, rebuildIndex, cleanTmp, writeVerifile, markEndTime];\n    const stats = {};\n\n    for (const step of steps) {\n      const label = step.name;\n      const start = new Date();\n      const s = yield step(cache, opts);\n\n      if (s) {\n        Object.keys(s).forEach(k => {\n          stats[k] = s[k];\n        });\n      }\n\n      const end = new Date();\n\n      if (!stats.runTime) {\n        stats.runTime = {};\n      }\n\n      stats.runTime[label] = end - start;\n    }\n\n    stats.runTime.total = stats.endTime - stats.startTime;\n    opts.log.silly('verify', 'verification finished for', cache, 'in', `${stats.runTime.total}ms`);\n    return stats;\n  });\n  return _verify.apply(this, arguments);\n}\n\nfunction markStartTime(_x3, _x4) {\n  return _markStartTime.apply(this, arguments);\n}\n\nfunction _markStartTime() {\n  _markStartTime = _asyncToGenerator(function* (cache, opts) {\n    return {\n      startTime: new Date()\n    };\n  });\n  return _markStartTime.apply(this, arguments);\n}\n\nfunction markEndTime(_x5, _x6) {\n  return _markEndTime.apply(this, arguments);\n}\n\nfunction _markEndTime() {\n  _markEndTime = _asyncToGenerator(function* (cache, opts) {\n    return {\n      endTime: new Date()\n    };\n  });\n  return _markEndTime.apply(this, arguments);\n}\n\nfunction fixPerms(_x7, _x8) {\n  return _fixPerms.apply(this, arguments);\n} // Implements a naive mark-and-sweep tracing garbage collector.\n//\n// The algorithm is basically as follows:\n// 1. Read (and filter) all index entries (\"pointers\")\n// 2. Mark each integrity value as \"live\"\n// 3. Read entire filesystem tree in `content-vX/` dir\n// 4. If content is live, verify its checksum and delete it if it fails\n// 5. If content is not marked as live, rimraf it.\n//\n\n\nfunction _fixPerms() {\n  _fixPerms = _asyncToGenerator(function* (cache, opts) {\n    opts.log.silly('verify', 'fixing cache permissions');\n    yield fixOwner.mkdirfix(cache, cache); // TODO - fix file permissions too\n\n    yield fixOwner.chownr(cache, cache);\n    return null;\n  });\n  return _fixPerms.apply(this, arguments);\n}\n\nfunction garbageCollect(_x9, _x10) {\n  return _garbageCollect.apply(this, arguments);\n}\n\nfunction _garbageCollect() {\n  _garbageCollect = _asyncToGenerator(function* (cache, opts) {\n    opts.log.silly('verify', 'garbage collecting content');\n    const indexStream = index.lsStream(cache);\n    const liveContent = new Set();\n    indexStream.on('data', entry => {\n      if (opts.filter && !opts.filter(entry)) {\n        return;\n      }\n\n      liveContent.add(entry.integrity.toString());\n    });\n    yield new Promise((resolve, reject) => {\n      indexStream.on('end', resolve).on('error', reject);\n    });\n    const contentDir = contentPath.contentDir(cache);\n    const files = yield glob(globify(path.join(contentDir, '**')), {\n      follow: false,\n      nodir: true,\n      nosort: true\n    });\n    const stats = {\n      verifiedContent: 0,\n      reclaimedCount: 0,\n      reclaimedSize: 0,\n      badContentCount: 0,\n      keptSize: 0\n    };\n    yield pMap(files, /*#__PURE__*/function () {\n      var _ref = _asyncToGenerator(function* (f) {\n        const split = f.split(/[/\\\\]/);\n        const digest = split.slice(split.length - 3).join('');\n        const algo = split[split.length - 4];\n        const integrity = ssri.fromHex(digest, algo);\n\n        if (liveContent.has(integrity.toString())) {\n          const info = yield verifyContent(f, integrity);\n\n          if (!info.valid) {\n            stats.reclaimedCount++;\n            stats.badContentCount++;\n            stats.reclaimedSize += info.size;\n          } else {\n            stats.verifiedContent++;\n            stats.keptSize += info.size;\n          }\n        } else {\n          // No entries refer to this content. We can delete.\n          stats.reclaimedCount++;\n          const s = yield fs.stat(f);\n          yield rimraf(f);\n          stats.reclaimedSize += s.size;\n        }\n\n        return stats;\n      });\n\n      return function (_x20) {\n        return _ref.apply(this, arguments);\n      };\n    }(), {\n      concurrency: opts.concurrency\n    });\n    return stats;\n  });\n  return _garbageCollect.apply(this, arguments);\n}\n\nfunction verifyContent(_x11, _x12) {\n  return _verifyContent.apply(this, arguments);\n}\n\nfunction _verifyContent() {\n  _verifyContent = _asyncToGenerator(function* (filepath, sri) {\n    const contentInfo = {};\n\n    try {\n      const {\n        size\n      } = yield fs.stat(filepath);\n      contentInfo.size = size;\n      contentInfo.valid = true;\n      yield ssri.checkStream(new fsm.ReadStream(filepath), sri);\n    } catch (err) {\n      if (err.code === 'ENOENT') {\n        return {\n          size: 0,\n          valid: false\n        };\n      }\n\n      if (err.code !== 'EINTEGRITY') {\n        throw err;\n      }\n\n      yield rimraf(filepath);\n      contentInfo.valid = false;\n    }\n\n    return contentInfo;\n  });\n  return _verifyContent.apply(this, arguments);\n}\n\nfunction rebuildIndex(_x13, _x14) {\n  return _rebuildIndex.apply(this, arguments);\n}\n\nfunction _rebuildIndex() {\n  _rebuildIndex = _asyncToGenerator(function* (cache, opts) {\n    opts.log.silly('verify', 'rebuilding index');\n    const entries = yield index.ls(cache);\n    const stats = {\n      missingContent: 0,\n      rejectedEntries: 0,\n      totalEntries: 0\n    };\n    const buckets = {};\n\n    for (const k in entries) {\n      /* istanbul ignore else */\n      if (hasOwnProperty(entries, k)) {\n        const hashed = index.hashKey(k);\n        const entry = entries[k];\n        const excluded = opts.filter && !opts.filter(entry);\n        excluded && stats.rejectedEntries++;\n\n        if (buckets[hashed] && !excluded) {\n          buckets[hashed].push(entry);\n        } else if (buckets[hashed] && excluded) {// skip\n        } else if (excluded) {\n          buckets[hashed] = [];\n          buckets[hashed]._path = index.bucketPath(cache, k);\n        } else {\n          buckets[hashed] = [entry];\n          buckets[hashed]._path = index.bucketPath(cache, k);\n        }\n      }\n    }\n\n    yield pMap(Object.keys(buckets), key => {\n      return rebuildBucket(cache, buckets[key], stats, opts);\n    }, {\n      concurrency: opts.concurrency\n    });\n    return stats;\n  });\n  return _rebuildIndex.apply(this, arguments);\n}\n\nfunction rebuildBucket(_x15, _x16, _x17, _x18) {\n  return _rebuildBucket.apply(this, arguments);\n}\n\nfunction _rebuildBucket() {\n  _rebuildBucket = _asyncToGenerator(function* (cache, bucket, stats, opts) {\n    yield fs.truncate(bucket._path); // This needs to be serialized because cacache explicitly\n    // lets very racy bucket conflicts clobber each other.\n\n    for (const entry of bucket) {\n      const content = contentPath(cache, entry.integrity);\n\n      try {\n        yield fs.stat(content);\n        yield index.insert(cache, entry.key, entry.integrity, {\n          metadata: entry.metadata,\n          size: entry.size\n        });\n        stats.totalEntries++;\n      } catch (err) {\n        if (err.code === 'ENOENT') {\n          stats.rejectedEntries++;\n          stats.missingContent++;\n        } else {\n          throw err;\n        }\n      }\n    }\n  });\n  return _rebuildBucket.apply(this, arguments);\n}\n\nfunction cleanTmp(cache, opts) {\n  opts.log.silly('verify', 'cleaning tmp directory');\n  return rimraf(path.join(cache, 'tmp'));\n}\n\nfunction writeVerifile(cache, opts) {\n  const verifile = path.join(cache, '_lastverified');\n  opts.log.silly('verify', 'writing verifile to ' + verifile);\n\n  try {\n    return fs.writeFile(verifile, `${Date.now()}`);\n  } finally {\n    fixOwner.chownr.sync(cache, verifile);\n  }\n}\n\nmodule.exports.lastRun = lastRun;\n\nfunction lastRun(_x19) {\n  return _lastRun.apply(this, arguments);\n}\n\nfunction _lastRun() {\n  _lastRun = _asyncToGenerator(function* (cache) {\n    const data = yield fs.readFile(path.join(cache, '_lastverified'), {\n      encoding: 'utf8'\n    });\n    return new Date(+data);\n  });\n  return _lastRun.apply(this, arguments);\n}","map":null,"metadata":{},"sourceType":"script"}